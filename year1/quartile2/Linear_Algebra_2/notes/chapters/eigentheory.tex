\section{Eigenvalues and Eigenvectors}

\subsection{Diagonalization of matrices}
\begin{definition}
    A square matrix $A$ has \emph{diagonal form} if all elements $a_{ij}$ with $i \neq j$ are zero.
\end{definition}

\begin{theorem}
    Let $\mathcal{A}: V \to V$ be a linear map and let $\alpha = \{\vec{a}_1, \dots, \vec{a}_n\}$ be a basis of $V$.
    The matrix $A_\alpha$ has a diagonal form
    $$A_\alpha = \begin{pmatrix}
        \lambda_1 & 0 & \dots & 0 \\
        0 & \lambda_2 & \ddots & \vdots \\
        \vdots & \ddots & \ddots & 0\\
    0 & \dots & 0 & \lambda_n\end{pmatrix}$$
    if and only if $\mathcal{A}\vec{a}_1 = \lambda_i\vec{a}_i$ for all $i \in \{1, \dots, n\}$.
\end{theorem}

\subsection{Eigenvalues and eigenvectors}

% \begin{definition}[Eigenvalue]
%     Let $\mathcal{A}: V \to V$ be a linear map. A scalar $\lambda \in \mathbb{K}$ is called an \emph{eigenvalue} of $\mathcal{A}$
%     if there exists a vector $\vec{v} \in V$ such that $\vec{v} \neq \vec{0}$ and $\mathcal{A}\vec{v} = \lambda\vec{v}$.
% \end{definition}

\begin{definition}[Eigenvector and eigenvalue]
    Let $\mathcal{A}: V \to V$ be a linear map from a $\mathbb{K}$-vector space $V$ to itself. A vector $\vec{v} \ne \vec{0} \in V$ is called an \emph{eigenvector} of $\mathcal{A}$
    with \emph{eigenvalue} $\lambda$ if $\mathcal{A}\vec{v} = \lambda\vec{v}$. We denote the set of all eigenvalues of
    $\mathcal{A}$ by $\spec{\mathcal{A}}$ and call it the \emph{spectrum} of $\mathcal{A}$.
\end{definition}

\begin{theorem}
    Let $\mathcal{A}: V \to V$ be a linear map with representation matrix $A_\alpha$ for a basis $\alpha$. Then $A_\alpha$
    is in diagonal form if and only if $\alpha$ is a basis of eigenvectors of $\mathcal{A}$. In this case, the diagonal
    entries of $A_\alpha$ are the eigenvalues of $\mathcal{A}$.
\end{theorem}

\begin{definition}[Eigenspace]
    Let $\mathcal{A}: V \to V$ be a linear map. For any scalar $\lambda \in \mathbb{K}$, we denote
    $$E_\lambda := \mathcal{N}(\mathcal{A} - \lambda\mathcal{I})$$
    Since \emph{null spaces} are subspaces, $E_\lambda$ is a subspace, called \emph{the eigenspace} of $\mathcal{A}$ for $\lambda$.
\end{definition}

\begin{remark}
Eigenspaces \emph{indeed are spaces of eigenvectors for a given eigenvalue}: $E_\lambda$ is the null space of the linear map $\mathcal{A} - \lambda \mathcal{I}$.
\begin{itemize}
    \item $\vec{v} \in E_\lambda \iff (\mathcal{A}-\lambda\mathcal{I})\vec{v} = \vec{0}$.
    \item $\vec{v} \in E_\lambda \iff \mathcal{A}\vec{v} - \lambda\vec{v} = \vec{0}$.
\end{itemize}
So any vector $\vec{v}$ lies in $E_\lambda$ if and only if $(\mathcal{A}-\lambda\mathcal{I})\vec{v} = \vec{0}$, which is equivalent to $\mathcal{A}\vec{v} - \lambda\vec{v} = \vec{0}.$
\end{remark}

\begin{remark}[null space as eigenspace]
    We can also write the null space of $\mathcal{A}$ as an eigenspace: $E_0$ consists of vectors that are mapped to $0$ times itself, so on $\vec{0}$.
\end{remark}

\subsection{Computing eigenvalues and eigenspaces}

\begin{theorem}
    $\lambda$ is an eigenvalue if and only if $det(A - \lambda \Id) = 0$. Let $\alpha = \{\vec{a}_1,\dots, \vec{a}_n\}$ be a basis for $V$, and let
    $$A = \begin{pmatrix} a_{11} & a_{12} & \dots & a_{1n} \\ a_{21}  & a_{22}  & & \vdots \\ \vdots & & \ddots & \vdots \\ a_{n1} & \dots & \dots & a_{nn} \end{pmatrix}$$
    be the matrix of $\mathcal{A}$ w.r.t. this basis. Then the eigenvectors for eigenvalue $\lambda$, in $\alpha$-coordinates, are the non-zero solutions of the system
    $$\begin{pmatrix} a_{11}-\lambda & a_{12} & \dots & a_{1n} \\ a_{21} & a_{22} - \lambda & & \vdots \\ \vdots & & \ddots & \vdots \\ a_{n1} & \dots & \dots & a_{nn} - \lambda \end{pmatrix}
    \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix} $$
\end{theorem}

\subsection{Characteristic polynomial}
\begin{definition}[Characteristic polynomial]
    Let $\mathcal{A}: V \to V$ be a linear map and let $A_\alpha$ be the matrix of $\mathcal{A}$ w.r.t. a basis $\alpha$. We call the equation $\det(A_\alpha - \lambda \Id) = 0$
    the \emph{characteristic equation} of $A_\alpha$, and the left-hand side off this equation, $\det(A_\alpha - \lambda \Id)$, the \emph{characteristic polynomial} of $A_\alpha$.
\end{definition}

We also call them characteristic equation/polynomial of $\mathcal{A}$, and denote the characteristic polynomial by $\chi_\mathcal{A}$.

\begin{theorem}
    Let $\mathcal{A}: V \to V$ be a linear map, $\alpha$ and $\beta$ be two bases for $V$ and let $A_\alpha/A_\beta$ be the matrix of $\mathcal{A}$ w.r.t. a basis $\alpha/\beta$.
    Then $\det(A_\alpha - \lambda \Id) = \det(A_\beta - \lambda \Id)$.
\end{theorem}

\begin{remark}
    The characteristic polynomial is independent of the choice of basis.
\end{remark}

\begin{theorem}
    Let $\mathcal{A}: V \to V$ be a linear map on a vector space $V$ of dimension $n$, and let
    $$A = \begin{pmatrix} a_{11} & a_{12} & \dots & a_{1n} \\
                          a_{21} & a_{22} &       & \vdots \\
                          \vdots &        & \ddots & \vdots \\ a_{n1} & \dots & \dots & a_{nn} \end{pmatrix}$$
    be the matrix of $\mathcal{A}$ w.r.t. a basis $\alpha$. Then the characteristic polynomial $\chi_\mathcal{A}$ is a polynomial of degree (exactly) $n$, and of the following shape:
    $$\chi_\mathcal{A}(\lambda) = (-1)^n\lambda^n + (-1)^{n-1}(a_{11}+a_{22}+\dots+a_{nn})\lambda^{n-1} + \dots + c_1\lambda + c_0$$
    for some coefficients $c_0, c_1, \dots \in \K$.
\end{theorem}

\begin{definition}[Trace]
    The sum of the diagonal elements of a square matrix $A$ is called the \emph{trace} of the matrix $A$. We denote it by $\tr(A)$.
\end{definition}

\begin{theorem}
    Let $\mathcal{A}: V \to V$ be a linear map with $\dim(V) < \infty$. For every basis $\alpha$, the matrix $A_\alpha$
    \begin{enumerate}
        \item has the same trace, which we therefore also call the \emph{trace}  of $\mathcal{A}$, and denote it by $\tr(\mathcal{A})$.
        \item has the same determinant, which we therefore also call the \emph{determinant} of $\mathcal{A}$, and denote it by $\det(\mathcal{A})$. We have the identity $\det(\mathcal{A}) = c_0$,
            where $c_0$ is the constant coefficient of the characteristic equation.
    \end{enumerate}
\end{theorem}

\begin{theorem}
    Let $A$ be a square matrix with entries in $\K$, where $\K \in \{\C, \C\}$, with characteristic polynomial $\chi_A(\lambda)$. Then the
    \begin{itemize}
        \item trace of the matrix is the sum of the roots of $\chi_A$; and the
        \item determinant of the matrix is the product of the roots of $\chi_A$.
    \end{itemize}
\end{theorem}

\subsection{Linear independence of eigenvectors}
\begin{theorem}
    Let $\mathcal{A}: V \to V$ be a linear map and let $\vec{v}_1, \dots, \vec{v}_n$ be eigenvectors of $\mathcal{A}$ for \emph{mutually different} eigenvalues
    $\lambda_1, \dots, \lambda_n$. Then $\vec{v}_1, \dots, \vec{v}_n$ are linearly independent.
\end{theorem}

