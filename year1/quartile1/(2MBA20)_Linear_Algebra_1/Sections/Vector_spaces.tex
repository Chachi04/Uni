\section{Vector spaces and linear subspaces}


\subsection{Vector spaces and linear subspaces}
\begin{definition}[(Vector space)]
    Let $V$ be a non-empty set whose elements we'll call vectors and will denote like vectors $\vec{a}$.
    Suppose a map $V \times V \rightarrow V$ and a map $\mathbb{K} \times V \rightarrow V$ are given.
    Denote the result of applying the first map to $ \vec{u} \text{ and } \vec{v} $ by $ \vec{u} \oplus \vec{v} $.
    Denote the result of applying the second map to the scalar $ \lambda $ and $ u \in V $ by $ \lambda * \vec{u} $.
    Suppose these operations satisfy the vector space axioms for all $ \vec{u}, \vec{v} $ and $ \vec{w} \in V $ and $ \lambda, \mu \in \mathbb{K} $,
    then $V$ is called a $\mathbb{K}$-vector space or a vector space over $ \mathbb{K} $, the operation $ \oplus $ is called \emph{vector addition},
    and the operation * is called \emph{scalar multiplication}. In the case $ \mathbb{K} = \mathbb{R} $ we also speak of a \emph{real vector space}, in the
    case $ \mathbb{K} = \mathbb{C} $ - of a \emph{complex vector space}.
\end{definition}


\subsubsection{Axioms of vector space}
    \begin{enumerate}
        \item $ \vec{u} \oplus \vec{v} = \vec{v} \oplus \vec{u} $ (commutativity)
        \item $ (\vec{u} \oplus \vec{v}) \oplus \vec{w} = \vec{u} \oplus (\vec{v} \oplus \vec{w}) $ (associativity)
        \item There is a \emph{zero vector} $ \vec{0} $ with the property $ \vec{v} \oplus \vec{0} = \vec{v} $
        \item Every vector $ \vec{u} $ has an opposite $ -\vec{u} $ such that $ \vec{u} \oplus -\vec{u} = \vec{0} $
        \item $ 1 * \vec{u} = \vec{u} $
        \item $ (\lambda\mu)*\vec{u} = \lambda * (\mu * \vec{u}) $
        \item $ (\lambda + \mu)*\vec{u} = \lambda * \vec{u} + \mu * \vec{u} $ (distributivity)
        \item $ \lambda * (\vec{u} \oplus \vec{v}) = \lambda*\vec{u}+\lambda*\vec{v} $ (distributivity)
    \end{enumerate}


\subsubsection{Additional arithmetic rules}
\begin{enumerate}
    \item There is pricisely one zero vector in $V$
    \item The opposite of a vector is unique
    \item For every vector $ \vec{v} \in V $ we have $ 0\cdot\vec{v} = \vec{0} $
    \item For every scalar $ \lambda $ we have $ \lambda\cdot\vec{0} = \vec{0} $
\end{enumerate}


\subsection{Spans, linearly (in)dependant systems}
\begin{definition}[(Span of vectors)]
    Given $\vec{a}_1$, $\vec{a}_2$, $\dots$, $\vec{a}_n$ vectors in $V$.
    \newline Span of $\vec{a}_1$, $\vec{a}_2$, $\dots$, $\vec{a}_n$:
    $<\vec{a}_1, \vec{a}_2, \dots, \vec{a}_3> = \{\lambda_1\vec{a}_1 + \lambda_2\vec{a}_2 + \dots + \lambda_n\vec{a}_n \vert \lambda_1,\dots,\lambda_n \in \mathbb{R}\}$
\end{definition}

\begin{theorem}[(Spans are linear subspaces)]
    If $a_1, \dots, a_n$ are vectors in the vector space $V$, then $< a_1 , \dots, a_n >$ is a linear subspace of $V$ .
\end{theorem}
\begin{proof}[Proof]
    Of course, the span is non-empty (it contains the zero vector, obtained by taking all scalars equal to $0$). \par
    Now let $\vec{p}$ and $\vec{q}$ be vectors in $<\vec{a}_1,\dots,\vec{a}_n> $ and suppose
    $$ \vec{p} = p_1\vec{a}_1 + p_2\vec{a}_2 + \dots + p_n\vec{a}_n \text{ and } \vec{q} = q_1\vec{a}_1+q_2\vec{a}_2+\dots+q_n\vec{a}_n.$$
    Then
    \begin{align*}
         \vec{p}+\vec{q} &= (p_1\vec{a}_1 + p_2\vec{a}_2 + \dots + p_n\vec{a}_n) + (q_1\vec{a}_1+q_2\vec{a}_2+\dots+q_n\vec{a}_n) \\
         &=(p_1+q_1)\vec{a}_1 + \dots + (p_n+q_n)\vec{a}_n \in <\vec{a}_1,\dots,\vec{a}_n>
    \end{align*}
    Also, for every scalar $ \lambda $
    \begin{align*}
        \lambda{p} &= \lambda(p_1\vec{a}_1 + p_2\vec{a}_2 + \dots + p_n\vec{a}_n) \\
        &= \lambda p_1\vec{a}_1 + \dots + \lambda p_n\vec{a}_n \in <\vec{a}_1, \dots, \vec{a}_n>
    \end{align*}
    Thus every span is a linear subspace.
\end{proof}


\subsubsection{Span operations}
\begin{enumerate}
    \item Swap 2 vectors.
    \item Multiply $\underbar{a}_i$ by $\lambda \ne 0$.
    \item Add $\lambda\underbar{a}_i$ to $\underbar{a}_j$ $(i \ne j)$.
    \item insert/append $\underbar{0}$ or leave out $\underbar{0}$.
    \item insert/append $\lambda_1\underbar{a}_1 + \dots + \lambda_n\underbar{a}_n$.
    \item leave out $\underbar{a}_i$ if it can be written as a linear combination.
\end{enumerate}

\begin{theorem}[(Exchange theorem)]
    If $V = <\vec{a}_1,<\dots,\vec{a}_n$ and $ \vec{b} = \lambda_1\vec{a}_1+\dots+\lambda_i\vec{a}_i+\dots+\lambda_n\vec{a}_n $ with $\lambda_i\ne 0$ for some $i$, then
    $$ V = <\vec{a}_1,\dots,\vec{a}_n> = <\vec{a}_1,\dots,\vec{a}_{i-1},\vec{b},\vec{a}_{i+1},\dots,\vec{a}_n> $$
\end{theorem}


\subsubsection{Linear (in)dependence}
\begin{definition}[(Linear (in)dependent set of vectors)]
    A set or system of vectors $ \vec{a}_1,\dots,\vec{a}_n $ is called \emph{linearly dependent} if at least one of the vectors
    is a linear combination of the others. \par
    The vectors are called \emph{linearly independent} if none of the vectors is a linear combination of the others.
\end{definition}

\begin{definition}[(Linear (in)dependent vectors: practical version)]
    The system of vectors $ \vec{a}_1,\dots,\vec{a}_2 $ is linearly independent if and only if the only solution of the equation
    $$ \lambda_1\vec{a}_1 + \lambda_2\vec{a}_2 + \dots + \lambda_n\vec{a}_n = \vec{0} $$
    in $ \lambda_1,\lambda_2,\dots,\lambda_n $ is: $ \lambda_1=\lambda_2=\dots=\lambda_n=0 $. \par
    The system is called \emph{linearly dependent} if it is not linearly independent.
\end{definition}


\subsubsection{Example} The functions $\sin$ and $ \cos $ in the space of real functions $ \set{R} \rightarrow \set{R}$ are linearly independent. Suppose
$$ a\sin + b\cos = 0 \text{ (the zero function)} $$
Then, \emph{since this is an equality of functions}, we find that for every $ t \in \set{R} $ the relation $ a\sin{t} + b\cos{t} = 0 $ holds.
Now we choose a few 'smart' value for $t$ to deduce that deduce that $a$ and $b$ are $0$: for $t=0$ we get $ b\cos{0} = 0 $ so $ b=0 $, and
fot $ t=\frac{\pi}{2} $ we get $ a\sin{\frac{\pi}{2}} = 0 $, so $ a = 0 $.

\begin{theorem}[]
    Suppose $ V = <\vec{a}_1,\dots,\vec{a}_n> $ and suppose $ \vec{b}_1,\dots,\vec{b}_m $ is a linearly independent set of vectors in $V$. Then $ m \le n $.
\end{theorem}

\begin{theorem}[]
    If the vector space $V$ is the span of each of the systems of independent vectors $ \vec{a}_1,\dots,\vec{a}_n $ and $ \vec{b}_1,\dots,\vec{b}_m $, then $ n = m $.
\end{theorem}

\begin{definition}[(Basis and dimension)]
    A linearly independent set spanning a vector space $V$ is called a \emph{basis} of $V$.
    The number of elements in the basis is called the \emph{dimension} of V and is denoted as dim$(V)$ (or dim$_{\set{K}}(V)$ if we want to emphasize the scalars).
    If no finite basis of $V$ exists (and $V$ does not contain only the zero vector), then we say the $V$ is infinite-dimensional and write dim$(V) = \infty$.
    We define the dimension of a vector space consisting of the zero vector only to be 0.
\end{definition}


\subsubsection{Examples}
\begin{enumerate}[label=(\alph*)]
    \item Geometrically it is clear that $ \dim(E^3) = 3 $ and $ \dim(E^2) = 2 $
    \item In $ \set{R}^n $ the set containing the vectors: \begin{align*}
        \vec{e}_1 &= (1,0,0,\dots,0), \\
        \vec{e}_2 &= (0,1,0,\dots,0), \\
                  &\vdots \\
        \vec{e}_n &= (0,0,0,\dots,1)
    \end{align*}
    is a linearly independent set spanning $ \set{R}^n $. So $ \dim(\set{R}^n) = n $
    \item Let $ P_n $ be the set of all (real or complex) polynomials in x of degree at most n.
    Then $ P_n = <1,x,x^2,\dots,x^n> $. $ \dim(P_n) = n+1 $
    \item The space $P$ of all polynomials can be shown to have infinite dimension.
\end{enumerate}


\subsubsection{Finding bases}
\begin{theorem}[]
    If the set of vectors $ \{\vec{a}_1,\dots,\vec{a}_n\} $ in the vector space $V$ satisfies
    $$ a_1 \ne \vec{0}, a_2 \notin <a_1>, a_3 \notin <a_1,a_2>, \dots, a_n \notin <\vec{a}_1, \dots, \vec{a}_n>, $$
    then the vectors are linearly independent.
\end{theorem}


\subsection{Coordinates}
\begin{definition}[(Coordinates)]
    Let $ \underbar{v}_1, \underbar{v}_2, \dots, \underbar{v}_n $ be a basis of the vector space V
    over $\mathbb{K}$. If
    $$ \underbar{v} = \lambda_1\underbar{v}_1 + \dots + \lambda_n\underbar{v}_n, $$
    then the vector $ (\lambda_1, \lambda_2, \dots, \lambda_n) $ is called the \emph{coordinate vector} and is itself a vector in $\mathbb{K}^n$.

    \begin{nb}
        The coordinate vector of $\underbar{v}$ is unique.
    \end{nb}
\end{definition}

\subsubsection{Example}

\subsection{Constructing vector spaces}
